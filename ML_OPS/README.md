# ML Model Deployment Learning Path

This lesson follows a progressive approach to machine learning model deployment, starting with batch inference and advancing to real-time endpoints.

## Learning Sequence

### 1. Batch Inference
Start with batch inference deployment:
- Deploy your first batch inference model
- Model deploys to a data warehouse
- Focus on model-only deployment initially

### 2. Pipeline Deployment (MLOPS_101_ONLINE_INFERENCE)
Next, deploy the complete pipeline:
- Deploy both model AND entire preprocessing pipeline
- Raw data can be processed end-to-end
- Eliminates need for separate data cleaning tasks
- Includes automated preprocessing (e.g., one-hot encoding)

### 3. Real-Time Endpoint Deployment
Finally, deploy as a real-time inference endpoint:
- Enable millisecond response times
- Test with provided Streamlit code
- Achieve sub-100ms response times with optimized deployment method
