{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2mxxtyk5xiflzvbuafmg",
   "authorId": "4927852566776",
   "authorName": "CHASE",
   "authorEmail": "chase.romano@snowflake.com",
   "sessionId": "ea12396e-4007-4232-91de-79338247c18b",
   "lastEditTime": 1762959990969
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "_Imports"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nfrom datetime import datetime\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\nfrom snowflake.snowpark.functions import col, to_timestamp, min, max, month, dayofweek, dayofyear, avg, date_add, sql_expr\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.types import IntegerType\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\nfrom snowflake.ml.registry import Registry\n\n#Snowflake feature store\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nVERSION_NUM = '0'\nDB = 'Demo'\nSCHEMA = 'Public'\nCOMPUTE_WAREHOUSE = 'DEMO_WH'\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3cc2dff6-565c-4bb7-a0a7-b681f943ac75",
   "metadata": {
    "language": "python",
    "name": "_Load_Data"
   },
   "outputs": [],
   "source": "try:\n    print(\"Reading table data...\")\n    df = session.table(f\"{DB}.{SCHEMA}.MORTGAGE_LENDING_DEMO_DATA\")\n    df.show(5)\nexcept:\n    print(\"Table not found! Uploading data to snowflake table\")\n    df_pandas = pd.read_csv(\"MORTGAGE_LENDING_DEMO_DATA.csv.zip\")\n    session.write_pandas(df_pandas, \"MORTGAGE_LENDING_DEMO_DATA\", auto_create_table=True)\n    df = session.table(f\"{DB}.{SCHEMA}.MORTGAGE_LENDING_DEMO_DATA\")\n    df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83e4fb3f-820c-48e0-9945-6369ef578548",
   "metadata": {
    "language": "python",
    "name": "_Data_Overview"
   },
   "outputs": [],
   "source": "table_name = \"MORTGAGE_LENDING_DEMO_DATA\"\nrow_count = session.table(f'{DB}.{SCHEMA}.{table_name}').count()\ncolumns = session.table(f'{DB}.{SCHEMA}.{table_name}').columns\nschema = session.table(f'{DB}.{SCHEMA}.{table_name}').schema\n\nprint(f\"Table: {table_name}\")\nprint(f\"Row count: {row_count}\")\nprint(f\"Number of columns: {len(columns)}\")\nprint(f\"Columns: {columns}\")\nprint(\"\\nSchema:\")\nfor field in schema.fields:\n    print(f\"  {field.name}: {field.datatype}\")\n\nfull_data = session.table(table_name).to_pandas()\nprint(\"Full dataset shape:\", full_data.shape)\nprint(\"\\nTarget distribution:\")\nprint(full_data['MORTGAGERESPONSE'].value_counts(normalize=True))\n\nprint(\"\\nCategorical variables unique counts:\")\ncategorical_cols = ['LOAN_TYPE_NAME', 'LOAN_PURPOSE_NAME', 'COUNTY_NAME']\nfor col in categorical_cols:\n    print(f\"{col}: {full_data[col].nunique()} unique values\")\n    print(f\"  Top 5: {full_data[col].value_counts().head().to_dict()}\")\n    print()\n\nprint(\"Missing values in full dataset:\")\nprint(full_data.isnull().sum())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2838403-ace6-439a-8a49-2bf7f74ac82a",
   "metadata": {
    "name": "_Model_1",
    "collapsed": false
   },
   "source": "Let's build a basic model.  You can convert any snowflake table to a pandas dataframe and build within the compute pool.  We will do some more feture engineering later on but for now we will build a simple model.\n\n- to_pandas() brings the data in memory to the compute pool and allows for data scientists to use the tool of thier choice, with the scalable compute of Snowflake.  \n- We will use get_dummies ot one hot encode, leaving off county due to the amount of distinct values\n- Drop records with null values in applicant income.\n- Split data into test and train\n- Train an xgboost model"
  },
  {
   "cell_type": "code",
   "id": "8e7ba91c-9f15-4763-802d-781ee0a99c6a",
   "metadata": {
    "language": "python",
    "name": "_Train_base_Model"
   },
   "outputs": [],
   "source": "df = df.to_pandas()\n\ndf_ohe = pd.get_dummies(df, columns=['LOAN_TYPE_NAME',\n                                    'LOAN_PURPOSE_NAME'], drop_first=True)\n\n# Convert all boolean columns to integers\ndf_ohe = df_ohe.apply(lambda x: x.astype(int) if x.dtype == 'bool' else x)\ndf_ohe.columns = [re.sub(r'[^a-zA-Z0-9]+', '_', col.upper()) for col in df_ohe.columns]\n\ndf_ohe = df_ohe.dropna(subset=['APPLICANT_INCOME_000S'])\n\nx = df_ohe.drop(['MORTGAGERESPONSE','LOAN_ID','TS','COUNTY_NAME'],axis=1)\ny = df_ohe.MORTGAGERESPONSE\n\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,train_size=.70,random_state=1234)\n\n# Model params\nxgb_base = XGBClassifier(\n    max_depth=50,\n    n_estimators=3,\n    learning_rate = 0.75,\n    booster = 'gbtree')\n\n# Fit model\nxgb_base.fit(xtrain,ytrain)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6b7c12f-1abf-4896-a9c9-0165170eae32",
   "metadata": {
    "language": "python",
    "name": "_Model_Metrics"
   },
   "outputs": [],
   "source": "y_pred_train = xgb_base.predict(xtrain)\ny_pred_proba_train = xgb_base.predict_proba(xtrain)[:, 1]\n\naccuracy = accuracy_score(ytrain, y_pred_train)\nprecision = precision_score(ytrain, y_pred_train, average='weighted')\nrecall = recall_score(ytrain, y_pred_train, average='weighted')\nf1 = f1_score(ytrain, y_pred_train, average='weighted')\n\nmetrics_train = {\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall\": recall,\n    \"F1 Score\": f1,\n}\n\ny_pred = xgb_base.predict(xtest)\ny_pred_proba = xgb_base.predict_proba(xtest)[:, 1]\n\naccuracy = accuracy_score(ytest, y_pred)\nprecision = precision_score(ytest, y_pred, average='weighted')\nrecall = recall_score(ytest, y_pred, average='weighted')\nf1 = f1_score(ytest, y_pred, average='weighted')\n\nmetrics_test = {\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall\": recall,\n    \"F1 Score\": f1,\n}\nmetrics_train\nmetrics_test",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c651596e-6cb6-402f-9910-5b139fb9ce54",
   "metadata": {
    "name": "_Not_so_fast",
    "collapsed": false
   },
   "source": "### Looks good right? We got above 80% accuracy on train and 70% accuracy on test data!.....Not so fast\n- When train is that much better than test, that's a sign of overfitting\n- let's check the confusion matrix"
  },
  {
   "cell_type": "code",
   "id": "4bf10c9c-72a5-4bc1-923e-dd45b7a12434",
   "metadata": {
    "language": "python",
    "name": "_visualize_results"
   },
   "outputs": [],
   "source": "print(f\"\\nROC AUC Score: {roc_auc_score(ytest, y_pred_proba):.4f}\")\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(ytest, y_pred)\nplt.figure(figsize=(2, 1.5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3c1bb371-2195-46e9-9ed2-9859ffa74f74",
   "metadata": {
    "name": "_results_Model_1",
    "collapsed": false
   },
   "source": "Our model may be accurate overall check out our 0 predictions.  We are out here giving loans (predicted 1) to actuals of 0.  That's not good for business.  Although we are providing loans to the actuals 1's, what is worse in this case?  Having someone reach back out for a review of their application or providing a loan to someone who may default?  Let's still log this and use it as an example of how to deploy via our model registry and then we will make this better."
  },
  {
   "cell_type": "code",
   "id": "859ce155-2133-4c02-874f-6a7a0173d127",
   "metadata": {
    "language": "python",
    "name": "_Log_base_model"
   },
   "outputs": [],
   "source": "sample_data = x.sample(n=1)\nreg = Registry(session=session, database_name= DB, schema_name= SCHEMA)\n\nmodel_name = \"MORTGAGE_LENDING_MLOPS\"\n\ntitanic_model = reg.log_model(\n    model_name=model_name,\n    target_platforms=[\"WAREHOUSE\"],\n    version_name = \"imbalanced_model\",\n    model=xgb_base,\n    sample_input_data= sample_data,\n    metrics=metrics_test,\n    options = {\n  \"relax_version\": True\n    }\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "259ea041-fc7f-40a1-afee-9d211cd801d2",
   "metadata": {
    "language": "python",
    "name": "_Show_Recent_model"
   },
   "outputs": [],
   "source": "recent_model = reg.get_model(model_name).last()\nversion = recent_model.version_name\n\nsession.sql(f'''\nALTER MODEL {model_name} SET\n  DEFAULT_VERSION = {version}\n'''    \n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f47669c-ebcc-4808-b8e5-97027afcc4cb",
   "metadata": {
    "language": "python",
    "name": "_Write_pd_df"
   },
   "outputs": [],
   "source": "session.write_pandas(xtest, f'{DB}.{SCHEMA}.MORTAGE_TEST', auto_create_table=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4394aa10-41c8-4bbd-a7c5-cc402c9f63f2",
   "metadata": {
    "language": "python",
    "name": "_SQL_Pred"
   },
   "outputs": [],
   "source": "sql_predict = session.sql(\n    f'''\n    select *, round({DB}.{SCHEMA}.MORTGAGE_LENDING_MLOPS!predict_proba(\n    APPLICANT_INCOME_000S,\n    LOAN_AMOUNT_000S,\n    LOAN_TYPE_NAME_FHA_INSURED,\n    LOAN_TYPE_NAME_FSA_RHS_GUARANTEED,\n    LOAN_TYPE_NAME_VA_GUARANTEED,\n    LOAN_PURPOSE_NAME_HOME_PURCHASE,\n    LOAN_PURPOSE_NAME_REFINANCING\n):output_feature_0,2)\nas pred_response\nfrom {DB}.{SCHEMA}.MORTAGE_TEST\n''')\n\nsql_predict.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c571a434-e1a3-4d1f-a216-f94355a572b8",
   "metadata": {
    "name": "_Next_Steps",
    "collapsed": false
   },
   "source": "#### Current Model Issues Analysis\n1. Class Imbalance: 76.9% majority vs 23.1% minority  \n2. Poor Minority Class Recall \n3. ROC AUC is ok\n4. Limited Feature Engineering: Only basic label encoding and we dropped records\n5. No Hyperparameter Tuning\n\n#### Improvements\n1. Feature Engineering: Create ratio features, binning  \n2. Handle Class Imbalance\n3. Hyperparameter Optimization: Key XGBoost parameters  \n4. Better Evaluation: Focus on minority class metrics"
  },
  {
   "cell_type": "markdown",
   "id": "e4a999ae-0748-45b8-a2df-fe9318d9fe8c",
   "metadata": {
    "name": "_FE_SP",
    "collapsed": false
   },
   "source": "### Feature engineer with Snowpark\n\n- Extract values for time of year\n- Missing income flag\n- Income to loan ratio\n- Rather than encoding every county let's calculate aggregates like mean county income\n- High income flag\n"
  },
  {
   "cell_type": "code",
   "id": "51193f6a-431b-4285-9680-0c5336f5d4f0",
   "metadata": {
    "language": "python",
    "name": "_Feature_Engineering"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, median, when, lit, coalesce\n\ndf = session.table(f'{DB}.{SCHEMA}.MORTGAGE_LENDING_DEMO_DATA')\n\ndf.select(min('TS'), max('TS'))\n\n#Get current date and time\ncurrent_time = datetime.now()\ndf_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n\n#Find delta between latest existing timestamp and today's date\ntimedelta = current_time- df_max_time\n\n#Update timestamps to represent last ~1 year from today's date\ndf.select(min(date_add(to_timestamp(\"TS\"), timedelta.days-1)), max(date_add(to_timestamp(\"TS\"), timedelta.days-1)))\nmedian_income = df.select(median(col(\"APPLICANT_INCOME_000s\"))).collect()[0][0]\n#Create a dict with keys for feature names and values containing transform code\n\nfeature_eng_dict = dict()\n\nfeature_eng_dict[\"TIMESTAMP\"] = date_add(to_timestamp(\"TS\"), timedelta.days-1)\nfeature_eng_dict[\"MONTH\"] = month(\"TIMESTAMP\")\nfeature_eng_dict[\"DAY_OF_YEAR\"] = dayofyear(\"TIMESTAMP\") \nfeature_eng_dict[\"DOTW\"] = dayofweek(\"TIMESTAMP\")\n\n#Income and loan features\nfeature_eng_dict[\"MISSING_INCOME\"] = when(col(\"APPLICANT_INCOME_000s\").isNull(), lit(1)).otherwise(lit(0))\nfeature_eng_dict[\"LOAN_AMOUNT\"] = col(\"LOAN_AMOUNT_000s\")*1000\nfeature_eng_dict[\"INCOME\"] = coalesce(col(\"APPLICANT_INCOME_000s\"), lit(median_income))*1000\nfeature_eng_dict[\"INCOME_LOAN_RATIO\"] = col(\"INCOME\")/col(\"LOAN_AMOUNT\")\n\ncounty_window_spec = Window.partition_by(\"COUNTY_NAME\")\nfeature_eng_dict[\"MEAN_COUNTY_INCOME\"] = avg(\"INCOME\").over(county_window_spec)\nfeature_eng_dict[\"HIGH_INCOME_FLAG\"] = when(col(\"MISSING_INCOME\") == 1, lit(0)).otherwise((col(\"INCOME\")>col(\"MEAN_COUNTY_INCOME\")).astype(IntegerType()))\nds_sp = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values()).drop('TS','COUNTY_NAME')\n\nds_sp.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b8e9a8d-062b-43ef-baf3-324f339cd76a",
   "metadata": {
    "language": "python",
    "name": "_OHE_Snowpark"
   },
   "outputs": [],
   "source": "import snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.snowpark.types import StringType\n\nOHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if isinstance(col.datatype, StringType)]).columns\nOHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\n#Rename columns to avoid double nested quotes and white space chars\n# Rename columns to avoid special characters\nrename_dict = {}\n\nfor col in ds_sp_ohe.columns:\n    new_name = col.replace('\"', '').replace(\"'\", '')\n    new_name = re.sub(r'[^\\w]', '_', new_name)\n    new_name = new_name.upper()   \n    if new_name != col:\n        rename_dict[col] = new_name\n\nds_sp_ohe = ds_sp_ohe.rename(rename_dict)\nds_sp_ohe.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ee90f56-3fa6-4f58-95fe-14a1dd384a0c",
   "metadata": {
    "language": "python",
    "name": "_HP_Tuning"
   },
   "outputs": [],
   "source": "df_clean = ds_sp_ohe.to_pandas()\n\nX = df_clean.drop(['MORTGAGERESPONSE','LOAN_ID','TIMESTAMP'],axis=1)\ny = df_clean.MORTGAGERESPONSE\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nscale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\nprint(f\"Calculated scale_pos_weight: {scale_pos_weight:.3f}\")\n\nparam_grid = {\n    \"n_estimators\": [100],\n    \"learning_rate\": [0.1],\n    \"max_depth\": [1,2,3,4],\n    \"min_child_weight\": [1, 6]\n}\n\n\nmodel = XGBClassifier(objective='binary:logistic', \n                      eval_metric='logloss',\n                      scale_pos_weight=scale_pos_weight)\n\nxgb_improved = GridSearchCV(estimator=model, \n                           param_grid=param_grid)\n\nxgb_improved.fit(X_train, y_train)\n\ny_pred_improved = xgb_improved.predict(X_test)\ny_pred_proba_improved = xgb_improved.predict_proba(X_test)[:, 1]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bec52adc-59e3-485a-aa9a-1dac3aefe048",
   "metadata": {
    "language": "python",
    "name": "_Model_2_metrics"
   },
   "outputs": [],
   "source": "y_pred_train = xgb_improved.predict(X_train)\ny_pred_proba_train = xgb_improved.predict_proba(X_train)[:, 1]\n\naccuracy = accuracy_score(y_train, y_pred_train)\nprecision = precision_score(y_train, y_pred_train, average='weighted')\nrecall = recall_score(y_train, y_pred_train, average='weighted')\nf1 = f1_score(y_train, y_pred_train, average='weighted')\n\nmetrics_train = {\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall\": recall,\n    \"F1 Score\": f1,\n}\n\ny_pred = xgb_improved.predict(X_test)\ny_pred_proba = xgb_improved.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nmetrics_test = {\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall\": recall,\n    \"F1 Score\": f1,\n}\nmetrics_train\nmetrics_test\n\nprint(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(2, 1.5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1e6c164-fb27-4858-bb02-a205ae1b27e2",
   "metadata": {
    "name": "_Model2_Summary",
    "collapsed": false
   },
   "source": "We have now shown how you can easily build, iterate on, and deploy models.  Now let's dive in to some more advanced features and explain why they are important for large scale production ML workloads.  This will include.... \n\n- Feature Store\n- Experiment Tracking\n- Distributed multi node training with Ray\n- Real time inference."
  },
  {
   "cell_type": "markdown",
   "id": "847c690d-9b4f-4a8c-98bf-a8e116f78448",
   "metadata": {
    "name": "_Exp_Tracking",
    "collapsed": false
   },
   "source": "## Time for Feature Store, Experiment Tracking, and distributed HPO with Ray!\n\nExperiment Tracking provides a mechanism for creating experiments and logging runs within Snowflake from any development environment. This capability allows you to log key pieces of information regarding your model training runs such as model parameters and metrics. In the UI, you can deep dive into a particular run or compare multiple runs to find the optimal model.\nBelow we will train multiple models using distributed HPO and log results to the Experiment Tracker!"
  },
  {
   "cell_type": "code",
   "id": "10b51109-40d5-4c9c-b798-e57e4fce4947",
   "metadata": {
    "language": "python",
    "name": "_Feat_eng_FS"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n\ntry:\n    print(\"Reading table data...\")\n    df = session.table(f\"{DB}.{SCHEMA}.MORTGAGE_LENDING_DEMO_DATA\")\nexcept:\n    print(\"Table not found! Uploading data to snowflake table\")\n    df_pandas = pd.read_csv(\"MORTGAGE_LENDING_DEMO_DATA.csv.zip\")\n    session.write_pandas(df_pandas, f\"{DB}.{SCHEMA}.MORTGAGE_LENDING_DEMO_DATA\", auto_create_table=True)\n\ndf.select(min('TS'), max('TS'))\n\n#Get current date and time\ncurrent_time = datetime.now()\ndf_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n\n#Find delta between latest existing timestamp and today's date\ntimedelta = current_time- df_max_time\n\n#Update timestamps to represent last ~1 year from today's date\ndf.select(min(date_add(to_timestamp(\"TS\"), timedelta.days-1)), max(date_add(to_timestamp(\"TS\"), timedelta.days-1)))\n\n#Create a dict with keys for feature names and values containing transform code\n\nfeature_eng_dict = dict()\n\n#Timstamp features\nfeature_eng_dict[\"TIMESTAMP\"] = date_add(to_timestamp(\"TS\"), timedelta.days-1)\nfeature_eng_dict[\"MONTH\"] = month(\"TIMESTAMP\")\nfeature_eng_dict[\"DAY_OF_YEAR\"] = dayofyear(\"TIMESTAMP\") \nfeature_eng_dict[\"DOTW\"] = dayofweek(\"TIMESTAMP\")\n\n# df= df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\n\n#Income and loan features\nfeature_eng_dict[\"LOAN_AMOUNT\"] = col(\"LOAN_AMOUNT_000s\")*1000\nfeature_eng_dict[\"INCOME\"] = col(\"APPLICANT_INCOME_000s\")*1000\nfeature_eng_dict[\"INCOME_LOAN_RATIO\"] = col(\"INCOME\")/col(\"LOAN_AMOUNT\")\n\ncounty_window_spec = Window.partition_by(\"COUNTY_NAME\")\nfeature_eng_dict[\"MEAN_COUNTY_INCOME\"] = avg(col(\"INCOME\").cast(IntegerType())).over(county_window_spec).astype(IntegerType())\nfeature_eng_dict[\"HIGH_INCOME_FLAG\"] = (col(\"INCOME\")>col(\"MEAN_COUNTY_INCOME\")).astype(IntegerType())\nfeature_eng_dict[\"AVG_THIRTY_DAY_LOAN_AMOUNT\"] =  sql_expr(\"\"\"AVG(LOAN_AMOUNT) OVER (PARTITION BY COUNTY_NAME ORDER BY TIMESTAMP  \n                                                            RANGE BETWEEN INTERVAL '30 DAYS' PRECEDING AND CURRENT ROW)\"\"\")\n\ndf = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\ndf.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b23db59-bed8-41eb-8eb3-33fbf1ca42f6",
   "metadata": {
    "language": "python",
    "name": "_FS_Creation"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=DB, \n    name=SCHEMA, \n    default_warehouse=COMPUTE_WAREHOUSE,\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\nfs.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6b814ba-e765-4f6b-b971-757be02d9bd8",
   "metadata": {
    "language": "python",
    "name": "_Entities"
   },
   "outputs": [],
   "source": "#First try to retrieve an existing entity definition, if not define a new one and register\ntry:\n    #retrieve existing entity\n    loan_id_entity = fs.get_entity('LOAN_ENTITY') \n    print('Retrieved existing entity')\nexcept:\n#define new entity\n    loan_id_entity = Entity(\n        name = \"LOAN_ENTITY\",\n        join_keys = [\"LOAN_ID\"],\n        desc = \"Features defined on a per loan level\")\n    #register\n    fs.register_entity(loan_id_entity)\n    print(\"Registered new entity\")\n\n#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\nfeature_df = df.select([\"LOAN_ID\"]+list(feature_eng_dict.keys()))\nfeature_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f89c781-02ca-4ca3-87c2-31f1d75daf4d",
   "metadata": {
    "language": "python",
    "name": "_Feature_View"
   },
   "outputs": [],
   "source": "#define and register feature view\nloan_fv = FeatureView(\n    name=\"Mortgage_Feature_View\",\n    entities=[loan_id_entity],\n    feature_df=feature_df,\n    timestamp_col=\"TIMESTAMP\",\n    refresh_freq=\"1 day\")\n\n#add feature level descriptions\n\nloan_fv = loan_fv.attach_feature_desc(\n    {\n        \"MONTH\": \"Month of loan\",\n        \"DAY_OF_YEAR\": \"Day of calendar year of loan\",\n        \"DOTW\": \"Day of the week of loan\",\n        \"LOAN_AMOUNT\": \"Loan amount in $USD\",\n        \"INCOME\": \"Household income in $USD\",\n        \"INCOME_LOAN_RATIO\": \"Ratio of LOAN_AMOUNT/INCOME\",\n        \"MEAN_COUNTY_INCOME\": \"Average household income aggregated at county level\",\n        \"HIGH_INCOME_FLAG\": \"Binary flag to indicate whether household income is higher than MEAN_COUNTY_INCOME\",\n        \"AVG_THIRTY_DAY_LOAN_AMOUNT\": \"Rolling 30 day average of LOAN_AMOUNT\"\n    }\n)\n\nloan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)\n\nfs.list_feature_views()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9680110a-06b1-4312-a3fd-fb0e59579fd5",
   "metadata": {
    "language": "python",
    "name": "_Get_Dataset"
   },
   "outputs": [],
   "source": "ds = fs.generate_dataset(\n    name=f\"MORTGAGE_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n    spine_df=df.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE_NAME\",\"MORTGAGERESPONSE\"), #only need the features used to fetch rest of feature view\n    features=[loan_fv],\n    spine_timestamp_col=\"TIMESTAMP\",\n    spine_label_cols=[\"MORTGAGERESPONSE\"]\n)\n\n#Convert Dataset to Snowpark Dataframe\n\nds_sp = ds.read.to_snowpark_dataframe()\nds_sp.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f936773-98dc-4ca6-b424-d0b647cd5b41",
   "metadata": {
    "language": "python",
    "name": "_FV_OHE"
   },
   "outputs": [],
   "source": "import snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.snowpark.types import StringType\n\nOHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype ==StringType()]).columns\nOHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\n#Rename columns to avoid double nested quotes and white space chars\nrename_dict = {}\nfor i in ds_sp_ohe.columns:\n    if '\"' in i:\n        rename_dict[i] = i.replace('\"','').replace(' ', '_')\n\nds_sp_ohe = ds_sp_ohe.rename(rename_dict)\nds_sp_ohe.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a53bb13-0a30-48c7-abf4-09198d89b8ea",
   "metadata": {
    "language": "python",
    "name": "_Train_test"
   },
   "outputs": [],
   "source": "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)\ntrain = train.fillna(0)\ntest = test.fillna(0)\ntrain_pd = train.to_pandas()\ntest_pd = test.to_pandas()\n\n# Convert all boolean columns to integers\ntrain_pd = train_pd.apply(lambda x: x.astype(int) if x.dtype == 'bool' else x)\ntrain_pd.columns = [re.sub(r'[^a-zA-Z0-9]+', '_', col.upper()) for col in train_pd.columns]\n\ntest_pd = test_pd.apply(lambda x: x.astype(int) if x.dtype == 'bool' else x)\ntest_pd.columns = [re.sub(r'[^a-zA-Z0-9]+', '_', col.upper()) for col in test_pd.columns]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cbfd4be-ede2-4db3-b364-3b8fd2212a2b",
   "metadata": {
    "language": "python",
    "name": "_Base_Model"
   },
   "outputs": [],
   "source": "#Define model config\nxgb_base = XGBClassifier(\n    max_depth=50,\n    n_estimators=3,\n    learning_rate = 0.75,\n    booster = 'gbtree')\n\n#Split train data into X, y\nX_train_pd = train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1) #remove\ny_train_pd = train_pd.MORTGAGERESPONSE\n\n#train model\nxgb_base.fit(X_train_pd,y_train_pd)\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score\ntrain_preds_base = xgb_base.predict(X_train_pd) #update this line with correct ata\n\nf1_base_train = round(f1_score(y_train_pd, train_preds_base),4)\nprecision_base_train = round(precision_score(y_train_pd, train_preds_base),4)\nrecall_base_train = round(recall_score(y_train_pd, train_preds_base),4)\n\nprint(f'F1: {f1_base_train} \\nPrecision {precision_base_train} \\nRecall: {recall_base_train}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7297f3c9-c649-4a31-9370-b8a5ad460f6e",
   "metadata": {
    "language": "python",
    "name": "_Log_Base_Model"
   },
   "outputs": [],
   "source": "#Create a snowflake model registry object \nfrom snowflake.ml.registry import Registry\n\n# Define model name\nmodel_name = f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\"\n\n# Create a registry to log the model to\nmodel_registry = Registry(session=session, \n                          database_name=DB, \n                          schema_name=SCHEMA,\n                          options={\"enable_monitoring\": True})\n\n#Log the base model to the model registry (if not already there)\nbase_version_name = 'XGB_BASE'\n\ntry:\n    #Check for existing model\n    mv_base = model_registry.get_model(model_name).version(base_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    #Log model to registry\n    mv_base = model_registry.log_model(\n        model_name=model_name,\n        model=xgb_base, \n        version_name=base_version_name,\n        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100), #using snowpark df to maintain lineage\n        comment = f\"\"\"ML model for predicting loan approval likelihood.\n                    This model was trained using XGBoost classifier.\n                    Hyperparameters used were:\n                    max_depth={xgb_base.max_depth}, \n                    n_estimators={xgb_base.n_estimators}, \n                    learning_rate = {xgb_base.learning_rate}, \n                    algorithm = {xgb_base.booster}\n                    \"\"\",\n        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n        options= {\"enable_explainability\": True}\n\n    )\n    \n    #set metrics\n    mv_base.set_metric(metric_name=\"Train_F1_Score\", value=f1_base_train)\n    mv_base.set_metric(metric_name=\"Train_Precision_Score\", value=precision_base_train)\n    mv_base.set_metric(metric_name=\"Train_Recall_score\", value=recall_base_train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1ee5f4c-e019-4444-94c9-97e414a7881f",
   "metadata": {
    "language": "python",
    "name": "_Create_PROD_Tag"
   },
   "outputs": [],
   "source": "#Create tag for PROD model\nsession.sql(f'CREATE OR REPLACE TAG {DB}.{SCHEMA}.PROD')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ce417a2-3714-4d3e-b0b7-eb8f6040ff8e",
   "metadata": {
    "language": "python",
    "name": "_Apply_Tag"
   },
   "outputs": [],
   "source": "#Apply prod tag \nm = model_registry.get_model(model_name)\nm.comment = \"Loan approval prediction models\" #set model level comment\nm.set_tag(f'{DB}.{SCHEMA}.PROD', base_version_name)\nm.show_tags()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "624d6ff1-3205-42e7-adae-f2f38ff0a647",
   "metadata": {
    "language": "python",
    "name": "_Predict_Base"
   },
   "outputs": [],
   "source": "reg_preds = mv_base.run(test, function_name = \"predict\").rename(col('\"output_feature_0\"'), 'MORTGAGE_PREDICTION')\n\npreds_pd = reg_preds.select([\"MORTGAGERESPONSE\", \"MORTGAGE_PREDICTION\"]).to_pandas()\nf1_base_test = round(f1_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\nprecision_base_test = round(precision_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\nrecall_base_test = round(recall_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\n\n#log metrics to model registry model\nmv_base.set_metric(metric_name=\"Test_F1_Score\", value=f1_base_test)\nmv_base.set_metric(metric_name=\"Test_Precision_Score\", value=precision_base_test)\nmv_base.set_metric(metric_name=\"Test_Recall_score\", value=recall_base_test)\n\nprint(f'F1: {f1_base_train} \\nPrecision {precision_base_train} \\nRecall: {recall_base_train}')\nprint('----------')\nprint(f'F1: {f1_base_test} \\nPrecision {precision_base_test} \\nRecall: {recall_base_test}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "361b7d9e-e89b-4d96-b8cf-8660c3b1d509",
   "metadata": {
    "name": "_Overfit_ET",
    "collapsed": false
   },
   "source": "Model is still overfit let's use experiment tracking to help"
  },
  {
   "cell_type": "code",
   "id": "20464639-921c-4622-8b2f-abf796796bcd",
   "metadata": {
    "language": "python",
    "name": "_Exp_Track_Run"
   },
   "outputs": [],
   "source": "from snowflake.ml.data import DataConnector\nfrom snowflake.ml.modeling.tune import get_tuner_context\nfrom snowflake.ml.modeling import tune\nfrom entities import search_algorithm\nimport psutil\nfrom snowflake.ml.experiment.experiment_tracking import ExperimentTracking\nfrom snowflake.ml.runtime_cluster import get_ray_dashboard_url, scale_cluster\n\nst.write('Click the link below to view the ray cluster and follow along with your HPO job progress!')\nst.write('https://'+get_ray_dashboard_url())\n\n#Define dataset map\ndataset_map = {\n    \"x_train\": DataConnector.from_dataframe(train.drop(\"MORTGAGERESPONSE\", \"TIMESTAMP\", \"LOAN_ID\")),\n    \"y_train\": DataConnector.from_dataframe(train.select(\"MORTGAGERESPONSE\")),\n    \"x_test\": DataConnector.from_dataframe(test.drop(\"MORTGAGERESPONSE\",\"TIMESTAMP\", \"LOAN_ID\")),\n    \"y_test\": DataConnector.from_dataframe(test.select(\"MORTGAGERESPONSE\"))\n    }\n\n# Scale up the cluster\nscale_cluster(2)\n\n# Define a training function, with any models you choose within it.\ndef train_func():\n\n    local_session = get_active_session()\n    exp = ExperimentTracking(session=local_session)\n    \n    exp.set_experiment(\"E2E_MLOPS_HPO_Experiments\")\n    with exp.start_run():\n        # A context object provided by HPO API to expose data for the current HPO trial\n        \n        tuner_context = get_tuner_context()\n        \n        #Generate params\n        config = tuner_context.get_hyper_params()\n        dm = tuner_context.get_dataset_map()\n    \n        #Log params to experiment tracking\n        exp.log_params(config)\n        \n        #Instantiate mdoel with generated params\n        model = XGBClassifier(**config, random_state=42)\n    \n        X_train_pd = dm[\"x_train\"].to_pandas().sort_index()\n        y_train_pd = dm[\"y_train\"].to_pandas().sort_index()\n        X_test_pd = dm[\"x_test\"].to_pandas().sort_index()\n        y_test_pd = dm[\"y_test\"].to_pandas().sort_index()\n    \n        #Train model, get preds\n        model.fit(X_train_pd,y_train_pd)\n\n        #Run inference on train preds\n        train_preds = model.predict(X_train_pd)\n\n        #Run inference on test preds\n        test_preds = model.predict(X_test_pd)\n        \n        #compute metrics \n        f1_train = f1_score(y_train_pd,train_preds)\n        precision_train = precision_score(y_train_pd,train_preds)\n        recall_train = recall_score(y_train_pd,train_preds)\n\n        f1_test = f1_score(y_test_pd,test_preds)\n        precision_test = precision_score(y_test_pd,test_preds)\n        recall_test = recall_score(y_test_pd,test_preds)\n    \n        metrics_to_log = {\"F1_Train\": f1_train,\n                         \"Precision_Train\": precision_train,\n                         \"Recall_Train\": recall_train,\n                         \"F1_Test\": f1_test,\n                         \"Precision_Test\": precision_test,\n                         \"Recall_Test\": recall_test,}\n    \n        #Log metrics to experiment tracking and tuner context \n        exp.log_metrics(metrics_to_log)\n    \n        tuner_context.report(metrics=metrics_to_log, model=model)\n        \ntuner = tune.Tuner(\n    train_func=train_func,\n    search_space={\n        \"max_depth\": tune.randint(1, 30),\n        \"learning_rate\": tune.uniform(0.01, 0.5),\n        \"n_estimators\": tune.randint(50, 150),\n    },\n    tuner_config=tune.TunerConfig(\n        metric=\"F1_Test\",\n        mode=\"max\",\n        search_alg=search_algorithm.RandomSearch(random_state=101),\n        num_trials=4, #run 4 trial runs\n    ),\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b07aa90d-b819-44fa-a22e-b5c76301cf86",
   "metadata": {
    "language": "python",
    "name": "_Run_EXp_Dist_HPO"
   },
   "outputs": [],
   "source": "tuner_results = tuner.run(dataset_map=dataset_map)\ntuner_results.results",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dca442d-3313-4516-8c49-de7c4dc31f94",
   "metadata": {
    "language": "python",
    "name": "_Best_model"
   },
   "outputs": [],
   "source": "#Select best model results and inspect configuration\ntuned_model = tuner_results.best_model\ntuned_model",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40e3360a-04f8-4784-a683-610fbea8f343",
   "metadata": {
    "language": "python",
    "name": "_Model_Preds"
   },
   "outputs": [],
   "source": "#Generate predictions\nxgb_opt_preds = tuned_model.predict(train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1))\n\n#Generate performance metrics\nf1_opt_train = round(f1_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\nprecision_opt_train = round(precision_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\nrecall_opt_train = round(recall_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\n\nprint(f'Train Results: \\nF1: {f1_opt_train} \\nPrecision {precision_opt_train} \\nRecall: {recall_opt_train}')\n\n#Generate test predictions\nxgb_opt_preds_test = tuned_model.predict(test_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1))\n\n#Generate performance metrics on test data\nf1_opt_test = round(f1_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\nprecision_opt_test = round(precision_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\nrecall_opt_test = round(recall_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\n\nprint(f'Test Results: \\nF1: {f1_opt_test} \\nPrecision {precision_opt_test} \\nRecall: {recall_opt_test}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80cee712-54fb-4607-b99e-67386a6a5a53",
   "metadata": {
    "language": "python",
    "name": "_Log_HPO_Model"
   },
   "outputs": [],
   "source": "#Log the optimized model to the model registry (if not already there)\noptimized_version_name = 'XGB_Optimized'\n\ntry:\n    #Check for existing model\n    mv_opt = model_registry.get_model(model_name).version(optimized_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    #Log model to registry\n    print(\"Logging new model version...\")\n    mv_opt = model_registry.log_model(\n        model_name=model_name,\n        model=tuned_model, \n        version_name=optimized_version_name,\n        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100),\n        comment = f\"\"\"HPO ML model for predicting loan approval likelihood.\n            This model was trained using XGBoost classifier.\n            Optimized hyperparameters used were:\n            max_depth={tuned_model.max_depth}, \n            n_estimators={tuned_model.n_estimators}, \n            learning_rate = {tuned_model.learning_rate}, \n            \"\"\",\n        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n        options= {\"enable_explainability\": True}\n\n    )\n    #Set metrics\n    mv_opt.set_metric(metric_name=\"Train_F1_Score\", value=f1_opt_train)\n    mv_opt.set_metric(metric_name=\"Train_Precision_Score\", value=precision_opt_train)\n    mv_opt.set_metric(metric_name=\"Train_Recall_score\", value=recall_opt_train)\n\n    mv_opt.set_metric(metric_name=\"Test_F1_Score\", value=f1_opt_test)\n    mv_opt.set_metric(metric_name=\"Test_Precision_Score\", value=precision_opt_test)\n    mv_opt.set_metric(metric_name=\"Test_Recall_score\", value=recall_opt_test)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58cb7a33-e484-4dc5-9e9e-92dab312f992",
   "metadata": {
    "language": "python",
    "name": "_Base_Default"
   },
   "outputs": [],
   "source": "#Here we see the BASE version is our default version\nmodel_registry.get_model(model_name).default",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1be7ed86-4675-4942-8d9c-af64c9b5330c",
   "metadata": {
    "language": "python",
    "name": "_Set_Opt_Default"
   },
   "outputs": [],
   "source": "recent_model = model_registry.get_model(model_name).last()\nversion = recent_model.version_name\n\nsession.sql(f'''\nALTER MODEL {DB}.{SCHEMA}.{model_name} SET\n  DEFAULT_VERSION = {version}\n'''    \n).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a5db36e-df52-4122-bce3-b51534e3d37f",
   "metadata": {
    "language": "python",
    "name": "_Show_New_Default"
   },
   "outputs": [],
   "source": "model_registry.get_model(model_name).default",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1ba81bb-ba76-4cbf-92a9-db5363a21fad",
   "metadata": {
    "language": "python",
    "name": "_Update_Prod_Tag"
   },
   "outputs": [],
   "source": "#we'll now update the PROD tagged model to be the optimized model version rather than our overfit base version\nm.unset_tag(f'{DB}.{SCHEMA}.PROD')\nm.set_tag(f'{DB}.{SCHEMA}.PROD', optimized_version_name)\nm.show_tags()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4b8f61d-00a2-4ece-8153-386dc2acdc1b",
   "metadata": {
    "name": "_Explain",
    "collapsed": false
   },
   "source": "Explain our model"
  },
  {
   "cell_type": "code",
   "id": "64cf47c4-d1fa-4a83-a45c-54ae64fa3aad",
   "metadata": {
    "language": "python",
    "name": "_Shap"
   },
   "outputs": [],
   "source": "#create a sample of 1000 records\ntest_pd_sample=test_pd.rename(columns=rename_dict).sample(n=2500, random_state = 100).reset_index(drop=True)\n\n#Compute shapley values for each model\nbase_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\nopt_shap_pd = mv_opt.run(test_pd_sample, function_name=\"explain\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7a12737-60d3-44cd-a025-255de2fdbb15",
   "metadata": {
    "language": "python",
    "name": "_Show_Shap"
   },
   "outputs": [],
   "source": "from snowflake.ml.monitoring import explain_visualize\n\nfeat_df=test_pd_sample.drop([\"MORTGAGERESPONSE\",\"TIMESTAMP\", \"LOAN_ID\"],axis=1)\n\nexplain_visualize.plot_influence_sensitivity(base_shap_pd, feat_df, figsize=(750, 250))\n\n#Optionally test out other built-in functionality \n# explain_visualize.plot_force(base_shap_pd.iloc[0], feat_df.iloc[0], figsize=(1500, 500))\n# explain_visualize.plot_violin(base_shap_pd, feat_df, figsize=(1400, 100))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae60154f-4bd3-4cff-accf-fc450b0e7d0a",
   "metadata": {
    "language": "python",
    "name": "_Monitoring"
   },
   "outputs": [],
   "source": "train.write.save_as_table(f\"{DB}.{SCHEMA}.DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\ntest.write.save_as_table(f\"{DB}.{SCHEMA}.DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43e00b69-ae60-4adf-bfd6-9c0eefe5949f",
   "metadata": {
    "language": "python",
    "name": "_Create_Stage"
   },
   "outputs": [],
   "source": "session.sql(f'CREATE stage IF NOT EXISTS {DB}.{SCHEMA}.ML_STAGE').collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d27489a-8a43-4ff9-b0d4-7f31638670bc",
   "metadata": {
    "language": "python",
    "name": "_Inference_SPROC"
   },
   "outputs": [],
   "source": "from snowflake import snowpark\n\ndef demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n\n    reg = Registry(session=session)\n    m = reg.get_model(model_name)  # Fetch the model using the registry\n    mv = m.version(modelversion)\n    \n    input_table_name=table_name\n    pred_col = f'{modelversion}_PREDICTION'\n\n    # Read the input table to a dataframe\n    df = session.table(input_table_name)\n    results = mv.run(df, function_name=\"predict\").select(\"LOAN_ID\",'\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n    # 'results' is the output DataFrame with predictions\n\n    final = df.join(results, on=\"LOAN_ID\", how=\"full\")\n    # Write results back to Snowflake table\n    final.write.save_as_table(table_name, mode='overwrite',enable_schema_evolution=True)\n\n    return \"Success\"\n\n# Register the stored procedure\nsession.sproc.register(\n    func=demo_inference_sproc,\n    name=\"model_inference_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=StringType()\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27df42a5-b232-4751-81d3-b50aa9a4c74e",
   "metadata": {
    "language": "sql",
    "name": "_inf1"
   },
   "outputs": [],
   "source": "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7cfe1369-80e4-4a23-8d2d-8356a71c7d20",
   "metadata": {
    "language": "sql",
    "name": "_inf2"
   },
   "outputs": [],
   "source": "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36764a7d-a756-4aea-8e3d-74fb49257aab",
   "metadata": {
    "language": "sql",
    "name": "_inf3"
   },
   "outputs": [],
   "source": "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f754355b-c2ad-4ece-879b-2e8cc277048f",
   "metadata": {
    "language": "sql",
    "name": "_inf4"
   },
   "outputs": [],
   "source": "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d02afb7-7878-4a64-8459-005cb7c91c83",
   "metadata": {
    "language": "sql",
    "name": "_Run"
   },
   "outputs": [],
   "source": "select TIMESTAMP, LOAN_ID, INCOME, LOAN_AMOUNT, XGB_BASE_PREDICTION, XGB_OPTIMIZED_PREDICTION, MORTGAGERESPONSE \nFROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} \nlimit 20",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e8589ce-f87d-4363-892e-a7911fdc3a13",
   "metadata": {
    "language": "sql",
    "name": "_Test_monitor"
   },
   "outputs": [],
   "source": "ALTER TABLE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n\n\nUPDATE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nSET LOAN_PURPOSE = CASE\n    WHEN LOAN_PURPOSE_NAME_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n    WHEN LOAN_PURPOSE_NAME_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n    WHEN LOAN_PURPOSE_NAME_REFINANCING = 1 THEN 'REFINANCING'\n    ELSE 'OTHER'\nEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cab591ab-e418-4933-adab-9812cdf8c84f",
   "metadata": {
    "language": "sql",
    "name": "_Train_Monitor"
   },
   "outputs": [],
   "source": "ALTER TABLE DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n\n\nUPDATE DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\nSET LOAN_PURPOSE = CASE\n    WHEN LOAN_PURPOSE_NAME_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n    WHEN LOAN_PURPOSE_NAME_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n    WHEN LOAN_PURPOSE_NAME_REFINANCING = 1 THEN 'REFINANCING'\n    ELSE 'OTHER'\nEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2194756-14db-4332-87c9-44b967df8c74",
   "metadata": {
    "language": "sql",
    "name": "_Loan_Purpose"
   },
   "outputs": [],
   "source": "SELECT LOAN_PURPOSE_NAME_HOME_PURCHASE, LOAN_PURPOSE_NAME_HOME_IMPROVEMENT, LOAN_PURPOSE_NAME_REFINANCING, LOAN_PURPOSE FROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3508d26e-d14f-4160-8355-425d3c56b2cb",
   "metadata": {
    "language": "sql",
    "name": "_Create_Monitor"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_BASE_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{base_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_BASE_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(MORTGAGERESPONSE)\n    ID_COLUMNS=(LOAN_ID)\n    SEGMENT_COLUMNS = ('LOAN_PURPOSE')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "928e2cfc-0511-4393-a191-244ef5350359",
   "metadata": {
    "language": "sql",
    "name": "_Opt_Monitor"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_OPTIMIZED_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{optimized_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_OPTIMIZED_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(MORTGAGERESPONSE)\n    ID_COLUMNS=(LOAN_ID)\n    SEGMENT_COLUMNS = ('LOAN_PURPOSE')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95f3c12d-10cb-4ccc-9b26-00db121a4d54",
   "metadata": {
    "name": "_Service_Deployment",
    "collapsed": false
   },
   "source": "Deploy as a service"
  },
  {
   "cell_type": "code",
   "id": "1e4372e4-2b2e-4086-b5e8-33fdfc21f995",
   "metadata": {
    "language": "python",
    "name": "_Create_Pool_Hosting"
   },
   "outputs": [],
   "source": "# If you do not have a compute pool create one\nsession.sql(\n'''CREATE COMPUTE POOL IF NOT EXISTS mortgage_inference\n  MIN_NODES = 1\n  MAX_NODES = 2\n  INSTANCE_FAMILY = CPU_X64_S\n  '''\n).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0ebd1b5-4270-4495-ba71-d980636b6b62",
   "metadata": {
    "language": "python",
    "name": "_Image_Repo"
   },
   "outputs": [],
   "source": "# If you do not have an image repo create on\nsession.sql(f'''\nCREATE IMAGE REPOSITORY IF NOT EXISTS {DB}.{SCHEMA}.model_image_repo\n''').collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9467dcea-0c55-452c-842c-e884c173b2a9",
   "metadata": {
    "language": "python",
    "name": "_Service_Params"
   },
   "outputs": [],
   "source": "cp_name = \"mortgage_inference\"\nnum_spcs_nodes = '2'\nspcs_instance_family = 'CPU_X64_S'\nservice_name = 'MORTGAGE_LENDING_PREDICTION_SERVICE'\n\nextended_service_name = f'{DB}.{SCHEMA}.{service_name}'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe32c96-e558-4076-aa3f-7a8bf2609c8a",
   "metadata": {
    "language": "python",
    "name": "_Create_Service"
   },
   "outputs": [],
   "source": "#note this may take up to 5 minutes to run\n\nmv_opt.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes)\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa474baa-5a07-4e48-93bc-5f07630b23d2",
   "metadata": {
    "language": "python",
    "name": "_Show_Service_SQL"
   },
   "outputs": [],
   "source": "session.sql(\n    '''\n    SHOW SERVICES LIKE '%MORTGAGE_LENDING_PREDICTION_SERVICE%'\n    '''\n).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a87f96ec-75e4-4d9d-8846-6c4631d515c8",
   "metadata": {
    "language": "python",
    "name": "_Show_Services_Python"
   },
   "outputs": [],
   "source": "mv_opt.list_services()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "557ac369-c103-4f33-bee4-e745d3b215d2",
   "metadata": {
    "language": "python",
    "name": "_Run_Service"
   },
   "outputs": [],
   "source": "mv_container = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").default\nmv_container.run(test, function_name = \"predict\", service_name = \"MORTGAGE_LENDING_PREDICTION_SERVICE\").rename('\"output_feature_0\"', 'XGB_PREDICTION')",
   "execution_count": null
  }
 ]
}